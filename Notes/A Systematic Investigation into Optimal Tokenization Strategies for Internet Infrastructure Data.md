### **1. Introduction & Problem Statement: The Unseen Foundation of Network Intelligence**

The analysis of internet infrastructure data is undergoing a paradigm shift, with machine learning (ML) models becoming essential tools for network security and operations. To leverage these models, complex sequential data from protocols like BGP (AS Paths) and measurement platforms (IP traceroute paths) must first be converted into a machine-readable format. This foundational step, **tokenization**, bridges the gap between raw, domain-specific text and the sophisticated architectures of modern deep learning. While often treated as a solved pre-processing step, the choice of tokenization strategy has a profound and largely unexamined impact on a model's ability to learn, reason, and generalize.

This thesis argues that the networking research community, in its adoption of sequence modeling, has largely imported outdated or overly simplistic tokenization paradigms without a critical, systematic evaluation. Current approaches are ad-hoc and fall into three main categories:

1.  **The "Word-Level" Paradigm:** Pioneering work like **IP2Vec (Ring et al., 2017)** and the state-of-the-art **AP2Vec (Shapira & Shavitt, 2022)** treat each unique IP address or Autonomous System Number (ASN) as a single, atomic "word." This approach is fundamentally limited, creating massive vocabularies, failing to handle new entities (the "unknown token catastrophe"), and remaining blind to the rich, hierarchical structure encoded within the data itself.

2.  **The "Manual Segmentation" Paradigm:** More recent, sophisticated models like **FlowletFormer (Liu et al., 2025)** recognize the flaws of naive tokenization. Their solution is to use expert domain knowledge to manually parse network packets into their constituent protocol fields, treating these as "indivisible semantic units." This correctly identifies the problem but solves it with hand-crafted engineering, which may not be optimal or easily generalizable across different types of network data.

3.  **The "Feature-Based" Paradigm:** The most common approach in the literature involves bypassing the sequence entirely, instead extracting a set of hand-crafted features which are then fed to a classifier. This loses the raw sequential information and is constrained by the initial choice of features.

A significant gap exists between these practices and the state-of-the-art in tokenization research. Foundational studies in NLP and parallel scientific domains have now conclusively shown that tokenization is a rich field of study in its own right. Works by **Schmidt et al. (2024)**, **Seo et al. (2025)**, and **Dotan et al. (2024)** have collectively proven that a domain-specific, structurally-aware tokenizer is critical for model stability, accuracy, and efficiency.

Despite the foundational importance of this step, there has been **no systematic, comparative study of tokenization philosophies for internet infrastructure data.** We lack empirical answers to the most basic questions: For representing an IP path, is it better to learn statistical "phrases" from the raw string, or to deterministically parse its fields? Or is a hybrid approach superior?

This thesis will be the first to address this critical gap. By applying the rigorous, comparative methodology from modern NLP and bioinformatics, this research will provide a foundational analysis to determine the optimal tokenization strategies for internet data.

### **2. Literature Review: Defining the Methodological Landscape**

This review establishes the "gold standard" for tokenizer research from NLP and parallel domains, and then categorizes the existing networking literature by their data representation paradigm to highlight the specific research gap this thesis will address.

#### **Literature Comparison Table: Paradigms of Data Representation**

| Paper Title / Authors                                                                | Year      | Paradigm / Methodology                                                                                                                     | Key Contribution                                                                                                                            | Role in My Thesis / The GAP                                                                                                                                                                 |
| :----------------------------------------------------------------------------------- | :-------- | :----------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **"Tokenization Is More Than Compression"** (Schmidt et al.)                         | 2024      | **NLP Methodological Study:** Systematic comparison of tokenizer design stages.                                                            | Proved that **compression is not the goal;** structural coherence is key.                                                                   | **Provides the core scientific justification** for my thesis's focus on structure over simple efficiency.                                                                                   |
| **"How Does a Language-Specific Tokenizer Affect LLMs?"** (Seo et al.)               | 2025      | **NLP Intrinsic Analysis:** Compares generic vs. domain-specific tokenizers, measuring internal model behavior.                            | Showed that a domain-specific tokenizer creates a more **stable and reliable model** that is less "confused" by complex tasks.              | **Provides my primary evaluation metric.** Justifies using reconstruction loss as a proxy for a "good" tokenizer.                                                                           |
| **"Getting the most out of your tokenizer..."** (Dagan et al.)                       | 2024      | **NLP Domain Adaptation Study:** Investigates specializing tokenizers for a new domain (source code).                                      | Proved that specializing a tokenizer for a new domain offers significant efficiency gains with negligible performance cost.                 | **Provides the motivation for domain-specificity,** proving the effort is worthwhile.                                                                                                       |
| **"Effect of tokenization on transformers for biological sequences"** (Dotan et al.) | 2024      | **Bioinformatics Methodological Study:** Systematic comparison of tokenizers (BPE, WP, etc.) on DNA/protein sequences.                     | Demonstrated a "win-win" of **increased accuracy and efficiency** from data-driven, domain-specific tokenizers.                             | **Provides the direct methodological blueprint for my thesis.** It is the "existence proof" that this comparative study is a high-impact contribution in a complex scientific domain.       |
| **IP2Vec** (Ring et al.) & **AP2Vec** (Shapira & Shavitt)                            | 2017/2022 | **Word-Level Embedding:** Treats each full IP/ASN as a single, atomic "word."                                                              | Pioneering works that showed the viability of learning functional embeddings for internet entities from their context.                      | Represents the **"Pure Statistical (Word-Level)"** baseline in my experiments.                                                                                                              |
| **FlowletFormer** (Liu et al.)                                                       | 2025      | **Domain-Aware Segmentation:** Manually parses packets into protocol fields.                                                               | Validates that naive tokenization is suboptimal and proposes a **hand-crafted, structural** tokenization.                                   | Inspires the **"Pure Structural (Field-Based)"** philosophy in my experiments.                                                                                                              |
| **--- Your Thesis Proposal ---**                                                     | **2025**  | **Systematic Tokenizer Comparison:** Implements and compares three distinct tokenization philosophies across multiple internet data types. | A foundational, first-of-its-kind study to establish best practices for tokenizing internet data, and proposes a **novel Hybrid** approach. | Directly addresses the un-asked questions from the networking papers, using the rigorous methodology from the NLP/Bioinformatics papers to create a foundational guide for future research. |

### **3. Proposed Methodology: A Comparative Study of Tokenization Philosophies**

The core of this thesis is a large-scale, controlled experiment designed to compare three distinct philosophies of tokenization for internet infrastructure data. The goal is to determine which approach enables a model to best learn the underlying structure of the data.

#### **3.1. Data Sources and Corpora**
This study will utilize three distinct types of sequential internet data:
1.  **BGP AS Paths:** Sourced from **RIPE RIS**, representing the "language" of routing policy.
2.  **IPv4 Traceroute Paths:** Sourced from **RIPE Atlas** and **CAIDA**, including rich metadata like RTT. This represents the "language" of IPv4 data-plane paths.
3.  **IPv6 Traceroute Paths:** A parallel corpus to test generalization to different address structures.

Each dataset will be pre-processed and split into training (80%), validation (10%), and testing (10%) sets.

#### **3.2. The Three Tokenization Philosophies**

I will implement and compare three competing tokenizer architectures for each data type.

1.  **Philosophy A: Pure Statistical (Subword on Raw String)**
    *   **Method:** This approach treats the raw path as a simple string. A Byte-Pair Encoding (BPE) tokenizer will be trained directly on this string data.
    *   **What it Learns:** Common character sequences. For IPs, it learns prefixes (e.g., `"145.97."`). For AS Paths, it learns multi-ASN "phrases" (e.g., `"174 3356"`).
    *   **Hypothesis:** This data-driven approach will automatically discover the most statistically relevant semantic units.

2.  **Philosophy B: Pure Structural (Field-Based Parsing)**
    *   **Method:** Inspired by `FlowletFormer`, this tokenizer uses expert knowledge to deterministically parse each path. A traceroute hop is not a string, but a collection of fields.
    *   **Implementation:** The IP address (`from`) is split into four octet tokens (`"192"`, `"168"`, ...). The RTT (`rtt`) is discretized into a single categorical token (`"Token_RTT_METRO"`). The sequence is framed with structural tokens like `[HOP_START]` and `[HOP_END]`.
    *   **Hypothesis:** This structured, human-engineered representation will provide a clearer, more interpretable signal to the model.

3.  **Philosophy C: Hybrid (Structural + Statistical)**
    *   **Method:** This is a novel hybrid approach that combines the other two philosophies. The path is first parsed into its structural fields as in Philosophy B. Then, a pre-trained BPE tokenizer is applied *only to the sequence of IP octets* within each hop.
    *   **What it Learns:** It learns from the structured fields (like binned RTT) while also using BPE to automatically discover common multi-octet "phrases" (like `"192 168"`) within the IP address field.
    *   **Hypothesis:** This hybrid model will capture the richest representation by combining the benefits of explicit structural parsing with data-driven pattern discovery.

#### **3.3. The Evaluation Framework**

To provide a fair and consistent measure of each tokenizer's effectiveness, I will use a single, unsupervised model and task across all experiments.
*   **Model Architecture:** A simple **LSTM-based sequence-to-sequence autoencoder**.
*   **Evaluation Task:** The autoencoder will be trained on a **reconstruction task**. A lower reconstruction loss indicates that the tokenizer created a more effective and "learnable" representation.
*   **Evaluation Metrics:**
    1.  **Representational Quality:** Measured by the **average reconstruction loss** on the test set.
    2.  **Representational Efficiency:** Measured as the **Compression Factor** relative to a simple baseline.

The final results will be presented in a series of plots showing **Representational Quality vs. Efficiency**, allowing for a direct, empirical comparison of the three philosophies for each type of internet data.

### **4. Expected Contributions**

1.  **A Foundational, Systematic Study of Tokenization for Internet Data:** The first comprehensive comparison of modern tokenization philosophies (statistical, structural, and a novel hybrid) across BGP, IPv4, and IPv6 data.
2.  **The Establishment of Empirically-Backed Best Practices:** This thesis will produce a clear set of recommendations for researchers on how to select an appropriate tokenization strategy for different types of internet data.
3.  **A Methodological Bridge Between NLP and Networking Research:** This work will validate and adapt the rigorous comparative frameworks from NLP and bioinformatics, providing a blueprint for future networking research in representation learning.
4.  **A Publicly Available Suite of Trained Tokenizers and Baseline Models:** A key deliverable will be the open-sourcing of all trained tokenizers and baseline models to serve as a resource for the networking community and accelerate future research.


| Title                                                                         | Key Findings                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| ----------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| How Does a Language-Specific Tokenizer Affect LLMs?                           | This study reveals that extending an LLM's tokenizer for a specific language (Korean) significantly alters its internal behavior beyond just improving processing efficiency. While the standard English-centric "Base" tokenizer surprisingly achieved higher raw accuracy due to its limited vocabulary reducing synonym conflicts, the "Extended" tokenizer demonstrated superior stability and semantic understanding. Crucially, the Extended tokenizer exhibited lower confidence when generating incorrect tokens—thereby reducing the likelihood of "confident hallucinations"—and achieved lower cross-entropy loss on complex tasks, indicating that it processes difficult linguistic structures with less confusion and produces fewer nonsensical outputs than the byte-segmenting base model.<br>                                                                                                 |
| Tokenization Is More Than Compression                                         | This study systematically challenges the prevailing assumption that higher tokenization efficiency (compression) drives better LLM performance. By introducing PathPiece—a tokenizer designed to mathematically minimize the corpus token count—and training 64 language models, the authors demonstrated that there is no meaningful correlation between minimizing token count and downstream task accuracy. Instead, the study found that pre-tokenization decisions (specifically, enforcing splits on whitespace) are far more critical than the choice of segmentation algorithm (BPE, Unigram, or WordPiece) or the raw compression ratio. Ultimately, models benefit more from token boundaries that respect linguistic structure (like words) than from maximizing information density per token.                                                                                                      |
| Getting the most out of your tokenizer for pre-training and domain adaptation | This paper demonstrates that while most practitioners blindly reuse general-purpose tokenizers (like Llama 2's) for domain-specific tasks, performing a "tokenizer transplant" to a domain-specialized tokenizer (e.g., for code) yields significant efficiency gains in inference speed and context length without sacrificing accuracy. The authors find that while raw BPE (Identity) offers the best compression, it harms performance by obscuring syntactic boundaries, confirming that pre-tokenization rules that separate syntax from semantics are essential. Crucially, they establish that a pre-trained LLM can successfully adapt to a completely new tokenizer within approximately 50 billion tokens of fine-tuning, and that increasing vocabulary size beyond 32k–64k offers diminishing returns, trading slight speed gains for increased memory usage without improving model intelligence. |
| 6Diffusion: LM_IPv6_address_generation_method_based_on_diffusion-LM           | The 6Diffusion-LM employs a specialized "IPv6Tokenizer" that begins by ==expanding shorthand addresses into their full, 32-digit hexadecimal== representation to ensure uniformity. Instead of grouping characters via algorithms like BPE, the system ==segments the address into 32 atomic "nybbles"== (single hex digits) and creates "nested indices" by explicitly pairing each digit’s value with its absolute ==position== (0–31). This sequence is then projected into a continuous vector space via an embedding layer, allowing the model to learn the distinct semantic roles of digits—distinguishing between network prefixes and interface identifiers—based on their specific location within the address hierarchy.                                                                                                                                                                             |
| 6Former_Transformer-Based_IPv6_Address_Generation                             | 6Former uses a Byte-Level Tokenizer. It merges every two hex digits into a single token, resulting in a sequence of 16 tokens with a vocabulary of 256. This is a middle-ground approach designed to be computationally faster than 6Diffusion-LM while avoiding the massive vocabulary complexity of a full Hextet tokenizer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
